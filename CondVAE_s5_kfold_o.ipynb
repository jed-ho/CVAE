{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "160e0d6c",
   "metadata": {},
   "source": [
    "# Demonstration of anomaly detection with CVAE using DASHlink data\n",
    "\n",
    "**Author: Milad Memarzadeh (milad.memarzadeh@nasa.gov)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831f2c0d",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d3a6137f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix, precision_recall_fscore_support, roc_curve, precision_recall_curve, average_precision_score, f1_score, auc\n",
    "from sklearn.utils import shuffle\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b697530",
   "metadata": {},
   "source": [
    "# Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6cb4fb32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (142100, 3)\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the dataset\n",
    "#data_dir = '/home/adlink3/Downloads/yahoo_s5/A2Benchmark/'\n",
    "data_dir = 'C:/Users/jed95/Documents/GitHub/anomaly_detection/dataset/yahoo_s5/A2Benchmark'\n",
    "# Get the list of all CSV files in the directory\n",
    "file_list = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.csv')]\n",
    "\n",
    "# Load all files into a single DataFrame\n",
    "df_list = []\n",
    "for file in file_list:\n",
    "    df = pd.read_csv(file)\n",
    "    df_list.append(df)\n",
    "\n",
    "data = pd.concat(df_list, ignore_index=True)\n",
    "print(\"Data shape:\", data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9599316a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values: timestamp     0\n",
      "value         0\n",
      "is_anomaly    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values:\", data.isnull().sum())\n",
    "\n",
    "# For simplicity, drop missing values (if any)\n",
    "data.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4345488d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "data['value'] = scaler.fit_transform(data['value'].values.reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f665b01",
   "metadata": {},
   "source": [
    "# Create Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7b216f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences shape: (142091, 10)\n",
      "Sequence labels shape: (142091,)\n"
     ]
    }
   ],
   "source": [
    "def create_sequences(values, labels, sequence_length):\n",
    "    sequences = []\n",
    "    seq_labels = []\n",
    "    for i in range(len(values) - sequence_length + 1):\n",
    "        seq = values[i:i + sequence_length]\n",
    "        label = labels[i + sequence_length - 1]  # Label of the last element in the sequence\n",
    "        sequences.append(seq)\n",
    "        seq_labels.append(label)\n",
    "    return np.array(sequences), np.array(seq_labels)\n",
    "\n",
    "sequence_length = 10  # You can adjust this \n",
    "values = data['value'].values\n",
    "labels = data['is_anomaly'].values  # Assuming 'is_anomaly' is the label column\n",
    "\n",
    "sequences, seq_labels = create_sequences(values, labels, sequence_length)\n",
    "print(\"Sequences shape:\", sequences.shape)\n",
    "print(\"Sequence labels shape:\", seq_labels.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4a0c45",
   "metadata": {},
   "source": [
    "# Split data into labeled and unlabeled sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a52c4848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into labeled and unlabeled sets\n",
    "labeled_fraction = 0.5  # 10% labeled data\n",
    "num_labeled = int(len(sequences) * labeled_fraction)\n",
    "\n",
    "# Shuffle data before splitting\n",
    "indices = np.arange(len(sequences))\n",
    "np.random.shuffle(indices)\n",
    "sequences = sequences[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "labeled_data = sequences[:num_labeled]\n",
    "labeled_labels = labels[:num_labeled]\n",
    "\n",
    "unlabeled_data = sequences[num_labeled:]\n",
    "unlabeled_labels = labels[num_labeled:]  # For evaluation purposes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3d6284fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indices for normal and anomalous sequences\n",
    "#normal_indices = np.where(seq_labels == 0)[0]\n",
    "#anomalous_indices = np.where(seq_labels == 1)[0]\n",
    "#\n",
    "## Extract normal and anomalous sequences\n",
    "#normal_sequences = sequences[normal_indices]\n",
    "#normal_labels = seq_labels[normal_indices]\n",
    "#\n",
    "#anomalous_sequences = sequences[anomalous_indices]\n",
    "#anomalous_labels = seq_labels[anomalous_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5cb09988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split normal data\n",
    "#X_train_normal, X_test_normal, y_train_normal, y_test_normal = train_test_split(\n",
    "#    normal_sequences, normal_labels, test_size=0.51, random_state=42, stratify=normal_labels)\n",
    "#\n",
    "## Split anomalous data\n",
    "#X_train_anomalous, X_test_anomalous, y_train_anomalous, y_test_anomalous = train_test_split(\n",
    "#    anomalous_sequences, anomalous_labels, test_size=0.51, random_state=42, stratify=anomalous_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f65f3dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combine training data\n",
    "#X_train = np.concatenate([X_train_normal, X_train_anomalous], axis=0)\n",
    "#y_train = np.concatenate([y_train_normal, y_train_anomalous], axis=0)\n",
    "#\n",
    "## Combine test data\n",
    "#X_test = np.concatenate([X_test_normal, X_test_anomalous], axis=0)\n",
    "#y_test = np.concatenate([y_test_normal, y_test_anomalous], axis=0)\n",
    "#print(\"Training data shape:\", X_train.shape, y_train.shape)\n",
    "#\n",
    "#print(\"Test data shape:\", X_test.shape, y_test.shape)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "28af8376",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Shuffle data\n",
    "#\n",
    "#X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
    "#X_test, y_test = shuffle(X_test, y_test, random_state=42)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db1beef",
   "metadata": {},
   "source": [
    "# Convert Data to Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9a03c958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert training data to tensors\n",
    "#X_train_tensor = torch.tensor(X_train).float()\n",
    "#y_train_tensor = torch.tensor(y_train).long()\n",
    "#\n",
    "## Convert test data to tensors\n",
    "#X_test_tensor = torch.tensor(X_test).float()\n",
    "#y_test_tensor = torch.tensor(y_test).long()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50dc79a",
   "metadata": {},
   "source": [
    "# Create Data Loaders for Labeled and Unlabeled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "af172bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, sequences, labels=None):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq = torch.FloatTensor(self.sequences[idx])\n",
    "        if self.labels is not None:\n",
    "            label = torch.LongTensor([self.labels[idx]])\n",
    "            return seq, label\n",
    "        else:\n",
    "            return seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c21a408d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_size = 64  # Adjust as needed\n",
    "#\n",
    "## Training data loader\n",
    "#train_dataset = TimeSeriesDataset(X_train_tensor, y_train_tensor)\n",
    "#train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False) #TODO adjust Shuffle training data? \n",
    "#\n",
    "## Test data loader\n",
    "#test_dataset = TimeSeriesDataset(X_test_tensor, y_test_tensor)\n",
    "#test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73e288d",
   "metadata": {},
   "source": [
    "# Instantiate and Train the Conditional VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e9ede661",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim, num_classes=2):\n",
    "        super(VAE, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2)\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(64, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(64, latent_dim)\n",
    "        self.classifier = nn.Linear(64, num_classes)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder_input = nn.Linear(latent_dim + num_classes, 64)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "        \n",
    "    def forward(self, x, y_onehot=None):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        \n",
    "        class_logits = self.classifier(h)\n",
    "        if y_onehot is None:\n",
    "            y_onehot = F.softmax(class_logits, dim=1)\n",
    "            \n",
    "        z_cond = torch.cat([z, y_onehot], dim=1)\n",
    "        x_recon = self.decoder_input(z_cond)\n",
    "        x_recon = self.decoder(x_recon)\n",
    "        return x_recon, mu, logvar, class_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "38d82b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(x_recon, x, mu, logvar, class_logits, labels):\n",
    "    recon_loss = F.mse_loss(x_recon, x, reduction='mean')\n",
    "    kld = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    class_loss = F.cross_entropy(class_logits, labels.view(-1))\n",
    "    return recon_loss + kld + class_loss, recon_loss, kld, class_loss\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_recon_loss = 0\n",
    "    total_kld = 0\n",
    "    total_class_loss = 0\n",
    "    for batch in dataloader:\n",
    "        x_batch, y_batch = batch\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        x_recon, mu, logvar, class_logits = model(x_batch)\n",
    "        loss, recon_loss, kld, class_loss = loss_function(x_recon, x_batch, mu, logvar, class_logits, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_recon_loss += recon_loss.item()\n",
    "        total_kld += kld.item()\n",
    "        total_class_loss += class_loss.item()\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_recon_loss = total_recon_loss / len(dataloader)\n",
    "    avg_kld = total_kld / len(dataloader)\n",
    "    avg_class_loss = total_class_loss / len(dataloader)\n",
    "    return avg_loss, avg_recon_loss, avg_kld, avg_class_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "254caee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1/5\n",
      "--------------------------------\n",
      "Epoch 1, Train Loss: 0.0505, Val Loss: 0.0228, Val F1-Score: 0.4993\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[96], line 48\u001b[0m\n\u001b[0;32m     44\u001b[0m patience \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m  \u001b[38;5;66;03m# Early stopping patience\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m     train_loss, train_recon_loss, train_kld, train_class_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# Validation\u001b[39;00m\n\u001b[0;32m     51\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n",
      "Cell \u001b[1;32mIn[95], line 22\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, dataloader, optimizer)\u001b[0m\n\u001b[0;32m     20\u001b[0m loss, recon_loss, kld, class_loss \u001b[38;5;241m=\u001b[39m loss_function(x_recon, x_batch, mu, logvar, class_logits, y_batch)\n\u001b[0;32m     21\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 22\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     25\u001b[0m total_recon_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m recon_loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\jed95\\.conda\\envs\\cvae\\lib\\site-packages\\torch\\optim\\optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    371\u001b[0m             )\n\u001b[1;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jed95\\.conda\\envs\\cvae\\lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\jed95\\.conda\\envs\\cvae\\lib\\site-packages\\torch\\optim\\adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    152\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    155\u001b[0m         group,\n\u001b[0;32m    156\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    161\u001b[0m         state_steps)\n\u001b[1;32m--> 163\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\jed95\\.conda\\envs\\cvae\\lib\\site-packages\\torch\\optim\\adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    309\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 311\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jed95\\.conda\\envs\\cvae\\lib\\site-packages\\torch\\optim\\adam.py:432\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    430\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 432\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    434\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[0;32m    436\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set up k-fold cross-validation\n",
    "k_folds = 5\n",
    "skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store metrics\n",
    "fold_train_losses = []\n",
    "fold_val_losses = []\n",
    "fold_val_f1_scores = []\n",
    "\n",
    "# Convert labeled data to dataset\n",
    "full_dataset = TimeSeriesDataset(labeled_data, labeled_labels)\n",
    "\n",
    "# Define input dimensions\n",
    "input_dim = sequence_length # Since each sequence has length equal to window_size\n",
    "latent_dim = 20  # Adjust as needed\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(labeled_data, labeled_labels)):\n",
    "    print(f'\\nFold {fold+1}/{k_folds}')\n",
    "    print('--------------------------------')\n",
    "    \n",
    "    # Split data\n",
    "    train_data = labeled_data[train_idx]\n",
    "    train_labels = labeled_labels[train_idx]\n",
    "    val_data = labeled_data[val_idx]\n",
    "    val_labels = labeled_labels[val_idx]\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = TimeSeriesDataset(train_data, train_labels)\n",
    "    val_dataset = TimeSeriesDataset(val_data, val_labels)\n",
    "    \n",
    "    batch_size = 64  # Adjust as needed\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Initialize model and optimizer\n",
    "    model = VAE(input_dim, latent_dim).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    num_epochs = 10  # Adjust as needed\n",
    "    \n",
    "    best_val_f1 = 0\n",
    "    epochs_no_improve = 0\n",
    "    patience = 3  # Early stopping patience\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        train_loss, train_recon_loss, train_kld, train_class_loss = train_epoch(model, train_loader, optimizer)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_true = []\n",
    "        val_pred = []\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in val_loader:\n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                x_recon, mu, logvar, class_logits = model(x_batch)\n",
    "                loss, recon_loss, kld, class_loss = loss_function(x_recon, x_batch, mu, logvar, class_logits, y_batch)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                predictions = class_logits.argmax(dim=1)\n",
    "                val_true.extend(y_batch.cpu().numpy())\n",
    "                val_pred.extend(predictions.cpu().numpy())\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_f1 = f1_score(val_true, val_pred, average='macro')\n",
    "        \n",
    "        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val F1-Score: {val_f1:.4f}')\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            epochs_no_improve = 0\n",
    "            # Save the best model for this fold\n",
    "            torch.save(model.state_dict(), f'best_model_fold_{fold+1}.pth')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print('Early stopping!')\n",
    "                break\n",
    "    \n",
    "    # Load the best model for this fold\n",
    "    model.load_state_dict(torch.load(f'best_model_fold_{fold+1}.pth'))\n",
    "    \n",
    "    # Store metrics for this fold\n",
    "    fold_train_losses.append(train_loss)\n",
    "    fold_val_losses.append(avg_val_loss)\n",
    "    fold_val_f1_scores.append(best_val_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0a92ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.modelsCondVAE import *\n",
    "from source.utilsCondVAEs5 import *\n",
    "\n",
    "# Instantiate the model\n",
    "latent_dim = 10  # Adjust as needed\n",
    "num_param = 10    # Since we have univariate time series\n",
    "window_size = 1\n",
    "num_classes = 2  # Normal and Anomaly\n",
    "scale_flag = 0   # Use Sigmoid activation in the decoder\n",
    "\n",
    "model = VAE(latent_dim, num_param, window_size, num_classes, scale_flag).to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10  # Adjust as needed\n",
    "train_model_full(model, optimizer, train_loader, num_epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06774dd",
   "metadata": {},
   "source": [
    "# Evaluate the Model and Detect Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eed4ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_anomaly_scores(model, data_loader):\n",
    "    model.eval()\n",
    "    anomaly_scores = []\n",
    "    true_labels = []\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            X_batch = X_batch.unsqueeze(1).to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            x_rec, class_logits = model(X_batch)\n",
    "            y_probs = F.softmax(class_logits, dim=1)\n",
    "\n",
    "            # Reconstruction error\n",
    "            rec_error = torch.mean((X_batch - x_rec) ** 2, dim=[1, 2])\n",
    "\n",
    "            # Classification probability for anomaly class\n",
    "            anomaly_prob = y_probs[:, 1]\n",
    "\n",
    "            # Combine scores\n",
    "            anomaly_score = rec_error * anomaly_prob\n",
    "\n",
    "            anomaly_scores.extend(anomaly_score.cpu().numpy())\n",
    "            true_labels.extend(y_batch.cpu().numpy())\n",
    "            predictions.extend(torch.argmax(class_logits, dim=1).cpu().numpy())\n",
    "\n",
    "    return np.array(anomaly_scores), np.array(true_labels), np.array(predictions)\n",
    "\n",
    "# Compute anomaly scores\n",
    "train_anomaly_scores, train_true_labels, train_predictions = compute_anomaly_scores(model, train_loader)\n",
    "\n",
    "\n",
    "test_anomaly_scores, test_true_labels, test_predictions = compute_anomaly_scores(model, test_loader)\n",
    "#print(test_anomaly_scores)\n",
    "info = precision_recall_fscore_support(test_true_labels, test_predictions, pos_label=1)\n",
    "print(\"Precision = {}%, recall = {}% and F1-score = {}%\".format(np.round(info[0][1]*100, 2),\n",
    "                                                                np.round(info[1][1]*100, 2),\n",
    "                                                                np.round(info[2][1]*100, 2)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5d4b36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc0aa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the scale factor\n",
    "scale = 2  # You can adjust this value\n",
    "# Calculate the threshold\n",
    "threshold = np.mean(train_anomaly_scores) + scale * np.std(train_anomaly_scores)\n",
    "print(f\"Anomaly Detection Threshold: {threshold}\")\n",
    "# Classify test data based on threshold\n",
    "threshold_predictions = (test_anomaly_scores > threshold).astype(int)\n",
    "\n",
    "# Evaluate threshold-based predictions\n",
    "print(\"Threshold-based Classification Report:\")\n",
    "print(classification_report(test_true_labels, threshold_predictions, target_names=['Normal', 'Anomaly']))\n",
    "\n",
    "# Confusion matrix\n",
    "cm_threshold = confusion_matrix(test_true_labels, threshold_predictions)\n",
    "print(\"Threshold-based Confusion Matrix:\")\n",
    "print(cm_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d281fb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(train_anomaly_scores[train_true_labels == 0], bins=50, alpha=0.6, label='Normal')\n",
    "plt.hist(train_anomaly_scores[train_true_labels == 1], bins=50, alpha=0.6, label='Anomaly')\n",
    "plt.title('Anomaly Score Distribution')\n",
    "plt.xlabel('Anomaly Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01f7b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_anomaly_scores_scatter(anomaly_scores, true_labels, threshold):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(range(len(anomaly_scores)), anomaly_scores, c=true_labels, cmap='coolwarm', label='Data Point')\n",
    "    plt.axhline(threshold, color='red', linestyle='--', label='Threshold')\n",
    "    plt.title('Anomaly Scores Scatter Plot')\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Anomaly Score')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function\n",
    "plot_anomaly_scores_scatter(test_anomaly_scores, test_true_labels, threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4583f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set plot style\n",
    "sns.set(style='whitegrid')\n",
    "def plot_anomaly_score_histogram(anomaly_scores, true_labels, threshold):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(anomaly_scores[true_labels == 0], bins=50, color='green', label='Normal', kde=True, stat='density')\n",
    "    sns.histplot(anomaly_scores[true_labels == 1], bins=50, color='red', label='Anomaly', kde=True, stat='density')\n",
    "    plt.axvline(threshold, color='blue', linestyle='--', label='Threshold')\n",
    "    plt.title('Histogram of Anomaly Scores')\n",
    "    plt.xlabel('Anomaly Score')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function\n",
    "plot_anomaly_score_histogram(test_anomaly_scores, test_true_labels, threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1997ea62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Compute AUC-ROC\n",
    "auc_score = roc_auc_score(test_true_labels, test_anomaly_scores)\n",
    "print(f\"AUC-ROC Score: {auc_score:.4f}\")\n",
    "\n",
    "# Compute Precision-Recall Curve\n",
    "precision, recall, thresholds = precision_recall_curve(test_true_labels, test_anomaly_scores)\n",
    "ap_score = average_precision_score(test_true_labels, test_anomaly_scores)\n",
    "print(f\"Average Precision Score: {ap_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e176694",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(test_true_labels, test_anomaly_scores)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'AUC = {auc_score:.4f}')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.title('ROC Curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3706a868",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(test_true_labels, test_predictions, target_names=['Normal', 'Anomaly']))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(test_true_labels, test_predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Compute AUC-ROC\n",
    "auc_score = roc_auc_score(test_true_labels, test_anomaly_scores)\n",
    "print(f\"AUC-ROC Score: {auc_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9979b673",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(test_anomaly_scores[test_true_labels == 0], bins=50, alpha=0.6, label='Normal')\n",
    "plt.hist(test_anomaly_scores[test_true_labels == 1], bins=50, alpha=0.6, label='Anomaly')\n",
    "plt.axvline(threshold, color='red', linestyle='--', label='Threshold')\n",
    "plt.title('Anomaly Score Distribution with Threshold')\n",
    "plt.xlabel('Anomaly Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
