{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "160e0d6c",
   "metadata": {},
   "source": [
    "# Demonstration of anomaly detection with CVAE using DASHlink data\n",
    "\n",
    "**Author: Milad Memarzadeh (milad.memarzadeh@nasa.gov)**\n",
    "\n",
    "In this notebook, we first learn about CVAE (Convolutional Variational Auto-Encoder) and its different components. Then, we will apply CVAE to a dataset from DASHlink project (https://c3.ndc.nasa.gov/dashlink/projects/85/) for anomaly detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b33fbd",
   "metadata": {},
   "source": [
    "### Part 1. What is a Variational Auto-Encoder?\n",
    "\n",
    "In this part, we first learn about the Variational Auto-Encoders (VAEs) and their use cases. VAEs are the generalization of Auto-Encoders that learn a lower-dimensional representation of the data using nonlinear transformations through neural networks in an unsupervised fashion. They are sometimes referred to as nonlinear Principal Component Analysis (PCA). The consists of two main components: (1) an encoder, $q_{\\phi}(z \\mid x)$, that maps the input data $ùëã$ to a lower-dimensional space, $ùëç$ (which is also called latent space), and (2) a decoder, $p_{\\theta}(x \\mid z)$ that reconstructs the original data $\\hat{X}$ by sampling from the low-dimensional latent space. Both encoder and decoder are parameterized by neural networks. \n",
    "\n",
    "VAEs use a regularizer to control the complexity of the distribution of the learned representations in the latent space. the main goal is that with proper regularization of the posterior distribution of the data in the latent space, i.e., $q_{\\phi}(z \\mid x)$, the model will be able to generalize better to an unseen data. They are trained based on two optimization objectives: (1) to optimize the quality/fidelity of the reconstructed data compared to the original ones, and (2) to regularize the posterior distribution of the latent space to not form a very complex distribution. This objective does not allow the model to overfit to the training data and be able to generalize to an unseen data with higher accuracy. The first objective is imposed by minimizing the reconstruction error, which can be calculated as a mean squared error between the input and the output, i.e., $\\lVert X - \\hat{X} \\rVert_{2}^{2}$ or maximizing the log-likelihood of the generated data. The second objective is imposed by minimizing the KL-divergence between the posterior distribution of the latent feature space and its prior distribution. In most cases, the prior/posteriors are assumed to form a multivariate Gaussian distribution. Given this, the overal objective function is defined as follows,\n",
    "\n",
    "$$\\mathcal{L}( \\theta, \\phi ; \\beta, x, z) = \\mathbb{E}_{q_{\\phi}(z \\mid x)}[\\log p_{\\theta}(x \\mid z) - \\beta \\text{KL}(q_{\\phi}(z \\mid x) \\Vert p(z))]$$\n",
    "\n",
    "where $\\beta$ controls the effect of regularization term on the overal training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5851a5",
   "metadata": {},
   "source": [
    "### Part 2. Convolutional VAE (CVAE)\n",
    "\n",
    "CVAE takes advantage of the convolutional neural networks to learn about the short-term as well as long-term temporal dependence in the input data. This is in comparison to fully connected neural networks that won't incorporate such temporal dependece. As a result, it is well-suited to work with multivariate time-series data. Figure below shows the illustration of the CVAE's architecture:\n",
    "\n",
    "\n",
    "<img src=\"cvae_arch.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "CVAE uses windowed time-series data as an input and applies series of convolutional operations with different filter sizes to take multiple local temporal dependencies into account. Then, the results of each series of convolutions are concatenated and mapped to the latent space. We use a similar architecture for both the encoder and the decoder. As a result, the decoder consists of a series of deconvolution and up-sampling with different filter sizes. Each branch of the encoder has four convolutional layers followed by a pooling layer, of which the number of input channels increases as the window size shrinks. Similarly, the decoder consists of four layers of de-convolution followed by an up-sampling layer, the number of input channels of which decreases as the window size expands.\n",
    "\n",
    "CVAE can be used for multiple purposes. It is an unsupervised reasoning model and does not require availability of labeled data for training. It is often used for:\n",
    "\n",
    "1. **Non-linear dimensionality reduction**: in this setting, it can be understood as a nonlinear Principal Component Analysis (PCA), where it uses series of nonlinear transformations to learn a lower-dimensional representation of the input data.\n",
    "\n",
    "2. **Anomaly detection**: if the training data consists mostly of nominal cases and few anomalies, one can imagine that training the model to reconstruct such data would only optimize this for the majority nominal data present in the training. Then, the reconstruction error can be used to detect those anomalous out-of-distribution data instances.\n",
    "\n",
    "\n",
    "3. **Data generation and augmentation**: once trained, CVAE can be used to generate and augment databases. One can perform random walks in the latent feature space and synthesize new data instances that are from the same distribution as the original input data.\n",
    "\n",
    "In this notebook, we will explore application of CVAE in finding anomalies in the final approach to landing of the commercial aircraft. We specifically focus on finding anomalies that are related to late deployment of the flaps, which results in a high speed and unstable approach to landing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831f2c0d",
   "metadata": {},
   "source": [
    "### Part 3. Final approach to landing data from DASHlink project\n",
    "\n",
    "The is a Flight Operational Quality Assurance (FOQA) data of a commerical airline from DASHlink project (link at the top of the notebook). It comprises primarily 1-Hz recordings for each flight and covers a variety of systems. These include: the state and orientation of the aircraft, positions and inputs of the control surfaces, engine parameters, and auto pilot modes and corresponding states. The data is acquired in real time on-board the aircraft and downloaded by the airline once the aircraft has reached the destination gate. These time series are analyzed by domain experts, which derive threshold-based rules post-flight to flag known events and create labels. Each data instance is a 160 s long recording of 10 variables during the approach of the aircraft to landing (from a few seconds before an altitude of 1000 ft to a few seconds after an altitude of 500 ft). \n",
    "\n",
    "As mentioned before, we will focus on finding anomalies that are related to late deployment of the flaps, which results in a high speed and unstable approach to landing. Let us take a look at a sample of nominal and anomalous data instances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3a6137f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "def load_yahoo_s5_dataset(path):\n",
    "    \"\"\"\n",
    "    Loads the Yahoo S5 dataset from the given path.\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to the Yahoo S5 dataset directory.\n",
    "\n",
    "    Returns:\n",
    "        list of dict: Each dict contains 'data', 'is_anomaly', and 'file_name'.\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "\n",
    "    # Get list of all CSV files in the dataset directories\n",
    "    csv_files = glob(os.path.join(path, 'A*Benchmark', '*.csv'))\n",
    "\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(file)\n",
    "        # Assuming the CSV has columns 'timestamp', 'value', 'is_anomaly'\n",
    "        data = df['value'].values.astype(float)\n",
    "        is_anomaly = df['is_anomaly'].values.astype(int)\n",
    "        data_list.append({\n",
    "            'data': data,\n",
    "            'is_anomaly': is_anomaly,\n",
    "            'file_name': os.path.basename(file)\n",
    "        })\n",
    "\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b697530",
   "metadata": {},
   "source": [
    "Now, let us load the entirety of the training and testing data and take a closer look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cb4fb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_normal_data(data_list):\n",
    "    \"\"\"\n",
    "    Concatenates all normal data from the dataset into a single array.\n",
    "\n",
    "    Args:\n",
    "        data_list (list of dict): List of data series with anomalies labeled.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Concatenated normal data.\n",
    "    \"\"\"\n",
    "    normal_data = []\n",
    "\n",
    "    for series in data_list:\n",
    "        data = series['data']\n",
    "        is_anomaly = series['is_anomaly']\n",
    "        normal_points = data[is_anomaly == 0]\n",
    "        normal_data.append(normal_points)\n",
    "\n",
    "    # Concatenate all normal data\n",
    "    normal_data = np.concatenate(normal_data)\n",
    "\n",
    "    return normal_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28b70c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def scale_data(data, scaler=None):\n",
    "    \"\"\"\n",
    "    Scales the data using the given scaler.\n",
    "\n",
    "    Args:\n",
    "        data (np.ndarray): Data to scale.\n",
    "        scaler (scaler object): Scaler object. If None, MinMaxScaler is used.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Scaled data.\n",
    "        scaler object: Scaler used.\n",
    "    \"\"\"\n",
    "    if scaler is None:\n",
    "        scaler = MinMaxScaler()\n",
    "        data = data.reshape(-1, 1)\n",
    "        scaled_data = scaler.fit_transform(data)\n",
    "        scaled_data = scaled_data.flatten()\n",
    "    else:\n",
    "        data = data.reshape(-1, 1)\n",
    "        scaled_data = scaler.transform(data)\n",
    "        scaled_data = scaled_data.flatten()\n",
    "    return scaled_data, scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f93ee7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, window_size):\n",
    "    \"\"\"\n",
    "    Creates sequences/windows from the data.\n",
    "\n",
    "    Args:\n",
    "        data (np.ndarray): Time series data.\n",
    "        window_size (int): Size of each window.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of shape (num_sequences, num_param, window_size)\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    for i in range(len(data) - window_size + 1):\n",
    "        seq = data[i:i+window_size]\n",
    "        sequences.append(seq)\n",
    "    sequences = np.array(sequences)\n",
    "    # Reshape to (num_sequences, num_param, window_size)\n",
    "    sequences = sequences.reshape(-1, 1, window_size)\n",
    "    return sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8723d894",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data(sequences, test_size=0.1):\n",
    "    \"\"\"\n",
    "    Splits the sequences into training and validation sets.\n",
    "\n",
    "    Args:\n",
    "        sequences (np.ndarray): Array of sequences.\n",
    "        test_size (float): Proportion of data to use for validation.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (train_sequences, val_sequences)\n",
    "    \"\"\"\n",
    "    train_sequences, val_sequences = train_test_split(sequences, test_size=test_size, shuffle=False)\n",
    "    return train_sequences, val_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6518b092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_sequences_with_labels(data_list, window_size, scaler):\n",
    "    \"\"\"\n",
    "    Creates test sequences from the data series, including labels.\n",
    "\n",
    "    Args:\n",
    "        data_list (list of dict): List of data series with anomalies labeled.\n",
    "        window_size (int): Size of each window.\n",
    "        scaler (scaler object): Scaler used for scaling the data.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (test_sequences, test_labels)\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for series in data_list:\n",
    "        data = series['data']\n",
    "        is_anomaly = series['is_anomaly']\n",
    "        # Scale the data using the existing scaler\n",
    "        data_scaled, _ = scale_data(data, scaler=scaler)\n",
    "        # Create sequences\n",
    "        seqs = create_sequences(data_scaled, window_size)\n",
    "        # For labels, if any point in the window is anomalous, label the sequence as anomalous\n",
    "        for i in range(len(data) - window_size + 1):\n",
    "            window_labels = is_anomaly[i:i+window_size]\n",
    "            label = 1 if window_labels.any() else 0\n",
    "            labels.append(label)\n",
    "        sequences.extend(seqs)\n",
    "    sequences = np.array(sequences)\n",
    "    labels = np.array(labels)\n",
    "    return sequences, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf1c2019",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        self.sequences = sequences.astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.sequences[idx]\n",
    "        return x\n",
    "\n",
    "def create_dataloader(sequences, batch_size=64, shuffle=True):\n",
    "    dataset = TimeSeriesDataset(sequences)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03b2d2a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'is_anomaly'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Preprocess the data\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m train_loader, val_loader, test_loader, test_labels \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_yahoo_s5\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 5\u001b[0m, in \u001b[0;36mpreprocess_yahoo_s5\u001b[1;34m(path, window_size, batch_size)\u001b[0m\n\u001b[0;32m      3\u001b[0m data_list \u001b[38;5;241m=\u001b[39m load_yahoo_s5_dataset(path)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Concatenate normal data\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m normal_data \u001b[38;5;241m=\u001b[39m \u001b[43mconcatenate_normal_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Scale the data\u001b[39;00m\n\u001b[0;32m      7\u001b[0m normal_data_scaled, scaler \u001b[38;5;241m=\u001b[39m scale_data(normal_data)\n",
      "Cell \u001b[1;32mIn[2], line 20\u001b[0m, in \u001b[0;36mconcatenate_normal_data\u001b[1;34m(data_list)\u001b[0m\n\u001b[0;32m     17\u001b[0m     normal_data\u001b[38;5;241m.\u001b[39mappend(normal_points)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Concatenate all normal data\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m normal_data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormal_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m normal_data\n",
      "\u001b[1;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "def preprocess_yahoo_s5(path, window_size, batch_size):\n",
    "    # Load the dataset\n",
    "    data_list = load_yahoo_s5_dataset(path)\n",
    "    # Concatenate normal data\n",
    "    normal_data = concatenate_normal_data(data_list)\n",
    "    # Scale the data\n",
    "    normal_data_scaled, scaler = scale_data(normal_data)\n",
    "    # Create sequences\n",
    "    sequences = create_sequences(normal_data_scaled, window_size)\n",
    "    # Split into training and validation sets\n",
    "    train_sequences, val_sequences = split_data(sequences)\n",
    "    # Create DataLoaders\n",
    "    train_loader = create_dataloader(train_sequences, batch_size, shuffle=True)\n",
    "    val_loader = create_dataloader(val_sequences, batch_size, shuffle=False)\n",
    "    # Prepare test data with labels\n",
    "    test_sequences, test_labels = create_test_sequences_with_labels(data_list, window_size, scaler)\n",
    "    test_dataset = TimeSeriesDataset(test_sequences)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, val_loader, test_loader, test_labels\n",
    "# Parameters\n",
    "dataset_path = 'C:/Users/jed95/Documents/GitHub/anomaly_detection/dataset/yahoo_s5/'  # Path to the dataset directory\n",
    "window_size = 50  # Adjust based on your needs\n",
    "batch_size = 128\n",
    "\n",
    "# Preprocess the data\n",
    "train_loader, val_loader, test_loader, test_labels = preprocess_yahoo_s5(dataset_path, window_size, batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341a8bd3",
   "metadata": {},
   "source": [
    "### Part 4. Training CVAE\n",
    "\n",
    "In this section, we show you steps that needs to be take to train the CVAE. Although, we will only load up a trained model later on, we will show you details of the steps here:\n",
    "\n",
    "\n",
    "Let us first start with preparing the data loaders for the model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9599316a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch\n",
    "#from source.modelsVAE import *\n",
    "#from source.utils import *\n",
    "##cuda = torch.cuda.is_available()\n",
    "## PyTorch wants the number of params to be the first dimension and the window size to be the second dimension\n",
    "#x_train = np.transpose(x_train, axes=(0, 2, 1))\n",
    "#x_test = np.transpose(x_test, axes=(0, 2, 1))\n",
    "#\n",
    "## Preparing training and testing datasets\n",
    "#train = Dataset(x_train)\n",
    "#test = Dataset(x_test)\n",
    "#\n",
    "## Preparing the data loader\n",
    "#batch_size = 128\n",
    "#train_data, test_data = get_dataset(train, test, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431aaa32",
   "metadata": {},
   "source": [
    "Now let us configure a model and train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4345488d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10 # Number of epochs for training\n",
    "latent_dim = 32 # dimension of the latent space\n",
    "beta = 0.001 # value of beta that controls the regularization\n",
    "metric = 'BCE' # choice of metric for calculating reconstruction error\n",
    "training = 1 # whether to train the model or load a trained one\n",
    "# Model parameters\n",
    "\n",
    "num_param = 1\n",
    "window_size = 50\n",
    "scale_flag = 0  # Use Sigmoid activation since data is scaled to [0, 1]\n",
    "\n",
    "# Naming the model for saving\n",
    "model_name = (\"s5_VAE_l_\"+(str(latent_dim))+\"_beta_\"+(str(beta))+\n",
    "              \"_batch_\"+str(batch_size)+\"_metric_\"+metric)\n",
    "\n",
    "# initiating the model\n",
    "model = VAE(latent_dim=latent_dim, num_param=num_param, window_size=window_size, scale_flag=scale_flag)\n",
    "\n",
    "# setting up the optimizer \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, betas=(0.9, 0.999))\n",
    "\n",
    "# training or loading the trained model:\n",
    "if training:\n",
    "    model = train_model(model, optimizer, model_name, train_loader, val_loader, './models/',\n",
    "                   metric, beta, num_epochs, save=True, verbose=1)\n",
    "    #model = model.to('cpu')\n",
    "else:\n",
    "    model.load_state_dict(torch.load((loading_dir+model_name+\".pth\"),\n",
    "                          map_location=torch.device('cpu')))\n",
    "    print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f665b01",
   "metadata": {},
   "source": [
    "### Part 5. Training statistics\n",
    "\n",
    "We have also saved the trajectory of training losses that we can visualize to make sure that the model has trained properly:\n",
    "\n",
    "**Note:** if you train a new model, this should be saved in your directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b216f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_traj = np.load(loading_dir+model_name+\"_training_loss.npz\")\n",
    "\n",
    "total_loss = training_traj['training_total_loss']\n",
    "rec_loss = training_traj['training_rec_loss']\n",
    "kl_loss = training_traj['training_kl_loss']\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 4))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Total Loss\", fontsize=12)\n",
    "plt.plot(range(num_epochs), total_loss)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Reconstruction Loss\", fontsize=12)\n",
    "plt.plot(range(num_epochs), rec_loss)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.xlabel(\"Epochs of training\", fontsize=12)\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"KL Loss\", fontsize=12)\n",
    "plt.plot(range(num_epochs), kl_loss)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af3951a",
   "metadata": {},
   "source": [
    "### Part 6. Performance evaluation on the testing set\n",
    "\n",
    "Now that we have trained a model, we will use it to identify anomalies in the testing set and evaluate how accurate we are. First, we need to calculate the threshold for anomaly detection based on reconstruction error of the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6284fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding a threshold for anomaly detection \n",
    "\n",
    "## Please note that this step might take a while. A smaller num_sample will\n",
    "### speed up this step with a trade-off of a less accurate estimate of the threshold.\n",
    "#### for the sake of time, we have saved the results of this step and will load it here:\n",
    "\n",
    "scale = 2 # how many std from mean should we use to set up the threshold\n",
    "\n",
    "train_anomaly_score = find_score(model, x_train, metric, num_sample=50)\n",
    "np.savez_compressed(loading_dir+model_name+\"_train_scores\", scores=train_anomaly_score)\n",
    "\n",
    "train_anomaly_score = np.load(loading_dir+model_name+\"_train_scores.npz\")['scores']\n",
    "threshold = np.mean(train_anomaly_score) + scale * np.std(train_anomaly_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62619074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once we find the threshold, we can use it to identify anomalies in the test set\n",
    "\n",
    "model_preds = np.zeros(np.shape(x_test)[0])\n",
    "\n",
    "# calculating the anomaly scores on the test set (again, for the sake of time, we will load this)\n",
    "test_anomaly_score = find_score(model, x_test, metric, num_sample=50)\n",
    "np.savez_compressed(loading_dir+model_name+\"_test_scores.npz\", scores=test_anomaly_score)\n",
    "\n",
    "test_anomaly_score = np.load(loading_dir+model_name+\"_test_scores.npz\")['scores']\n",
    "model_preds[test_anomaly_score > threshold] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4a0c45",
   "metadata": {},
   "source": [
    "Graph below, shows the histogram of the anomaly scores for the nominal and anomalous data samples in the testing set. We can see that for the nominal data, the score is generally lower and less variant, while it is much more variant and generally higher for the anomalous data samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c3dd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_nominal = np.where(y_test==0)[0]\n",
    "ind_anomaly = np.where(y_test==1)[0]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(test_anomaly_score[ind_nominal], bins=50, alpha=0.5, color='Green', label='Nominal')\n",
    "plt.hist(test_anomaly_score[ind_anomaly], bins=50, alpha=0.8, color='Orange', label='Anomalous')\n",
    "plt.vlines(threshold, 0, 300, color='blue', linestyle='dashed', alpha=0.5, label='Threshold')\n",
    "plt.ylim(0, 260)\n",
    "plt.xlabel(\"Anomaly Score\", fontsize=14)\n",
    "plt.legend(fontsize=14)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6175ea66",
   "metadata": {},
   "source": [
    "Below numbers are the precision, recall and F1-score of the performance on the testing set. They are defined as follow,\n",
    "\n",
    "\n",
    "$$ \\text{precision} = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "$$ \\text{recall} = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "where $TP$ is true positive: anomalies that are correctly identified, $FP$ is false positive (or false alarms):\n",
    "anomalies that are incorrectly identified, and $FN$ is false negative, or anomalies that are missed and classified as nominal by mistake.\n",
    "\n",
    "F1-score is the harmonic mean of the two above metrics and it is easier to choose, because one can makes the decision based on only one metric:\n",
    "\n",
    "$$ \\text{F1-score} = 2 \\times \\frac{\\text{precision} \\times \\text{recall}}{\\text{precision} + \\text{recall}}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f9b9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
    "info = precision_recall_fscore_support(y_test, model_preds, pos_label=1)\n",
    "print(\"Precision = {}%, recall = {}% and F1-score = {}%\".format(np.round(info[0][1]*100, 2),\n",
    "                                                                np.round(info[1][1]*100, 2),\n",
    "                                                                np.round(info[2][1]*100, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc344bd",
   "metadata": {},
   "source": [
    "Below, we also visualize the confusion matrix that shows the correct and wrong classifications for each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a03c958",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y_test, model_preds)\n",
    "\n",
    "\n",
    "FONT = 14\n",
    "classes = ['Nominal', 'Anomaly']\n",
    "#plt.figure(figsize=(8, 8))\n",
    "cmap = sns.cubehelix_palette(light=1, as_cmap=True)\n",
    "fig, ax1 = plt.subplots(1, figsize=(5, 5))\n",
    "\n",
    "im = ax1.imshow(cm, cmap=cmap)\n",
    "ax1.set_title(\"Confusion Matrix\", fontsize=FONT)\n",
    "ax1.set_xticks(np.arange(len(classes)))\n",
    "ax1.set_xticklabels(\"\")\n",
    "ax1.set_yticks(np.arange(len(classes)))\n",
    "ax1.set_xticklabels(classes, rotation=0)\n",
    "ax1.set_ylim(len(classes)-0.5, -0.5)\n",
    "ax1.set_yticklabels(classes)\n",
    "for i in range(len(classes)):\n",
    "    for j in range(len(classes)):\n",
    "        if cm[j, i] > 1000:\n",
    "            cc = 'white'\n",
    "        else:\n",
    "            cc = 'black'\n",
    "        text = ax1.text(i, j, cm[j, i],\n",
    "                       ha=\"center\", va=\"center\", color=cc, fontsize=FONT)\n",
    "ax1.set_ylabel(\"Actual Class\", fontsize=FONT)\n",
    "ax1.set_xlabel(\"Predicted Class\", fontsize=FONT)\n",
    "ax1.tick_params(axis='both', which='major', labelsize=FONT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a5a421",
   "metadata": {},
   "source": [
    "### Part 7. Structure of the data in the latent feature space\n",
    "\n",
    "In this part, we use t-Stochastic Neighbour Embedding (t-SNE) to visualize the 32-dimensional latent feature space in 2D and color code each data sample by the class it belongs to in order to better understand the structure of the learned feature space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "baf74ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "model.eval()\n",
    "\n",
    "# obtaining latent variables\n",
    "z_test = model.encoder(torch.from_numpy(x_test))[0].detach().numpy()\n",
    "\n",
    "# using t-SNE to map latnet variables to 2D\n",
    "tsne_test = TSNE(n_components=2, perplexity=50, init='pca', random_state=33).fit_transform(z_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af172bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['Green', 'Orange']\n",
    "labels = ['Nominal', 'Anomaly']\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Color-coded based on true labels\", fontsize=14)\n",
    "for i in range(2):\n",
    "    indices = np.where(y_test==i)[0]\n",
    "    plt.scatter(tsne_test[indices, 0], tsne_test[indices, 1], label=labels[i], c=colors[i], alpha=0.5)\n",
    "plt.legend(fontsize=12)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Color-coded based on model prediction\", fontsize=14)\n",
    "for i in range(2):\n",
    "    indices = np.where(model_preds==i)[0]\n",
    "    plt.scatter(tsne_test[indices, 0], tsne_test[indices, 1], label=labels[i], c=colors[i], alpha=0.5)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
