{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0753ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (56836, 10) (56836,)\n",
      "Validation data shape: (42627, 10) (42627,)\n",
      "Test data shape: (42628, 10) (42628,)\n",
      "Epoch 1 Loss 0.1097\n",
      "Epoch 2 Loss 0.0046\n",
      "Epoch 3 Loss 0.0012\n",
      "Epoch 4 Loss 0.0006\n",
      "Epoch 5 Loss 0.0005\n",
      "Uncompressed: 2273568 bytes\n",
      "Compressed: 2273568 bytes\n",
      "Ratio: 1.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from torchinfo import summary\n",
    "\n",
    "class Conv1dAutoencoder(nn.Module):\n",
    "    def __init__(self, input_length, latent_dim):\n",
    "        super(Conv1dAutoencoder, self).__init__()\n",
    "        self.input_length = input_length\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Encoder: Conv1d downsampling\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv1d(16, 8, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        # infer flattened size\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, 1, self.input_length)\n",
    "            enc = self.encoder(dummy)\n",
    "            c, l = enc.shape[1], enc.shape[2]\n",
    "            self._flattened_size = c * l\n",
    "\n",
    "        # Bottleneck\n",
    "        self.fc1 = nn.Linear(self._flattened_size, self.latent_dim)\n",
    "        self.fc2 = nn.Linear(self.latent_dim, self._flattened_size)\n",
    "\n",
    "        # Decoder: ConvTranspose1d upsampling\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Unflatten(1, (c, l)),\n",
    "            nn.ConvTranspose1d(c, 16, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose1d(16, 1, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc1(x)\n",
    "\n",
    "    def decode(self, z):\n",
    "        x = self.fc2(z)\n",
    "        x = self.decoder(x)\n",
    "        return x[..., :self.input_length]\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encode(x)\n",
    "        return self.decode(z), z\n",
    "\n",
    "# helper for file size\n",
    "\n",
    "def get_size(path):\n",
    "    try:\n",
    "        return os.path.getsize(path)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "# sequence creation\n",
    "\n",
    "def create_sequences(values, labels, window_size, step=1):\n",
    "    seqs, seq_labels = [], []\n",
    "    for i in range(0, len(values) - window_size + 1, step):\n",
    "        seq = values[i:i+window_size]\n",
    "        label = 1 if any(labels[i:i+window_size]) else 0\n",
    "        seqs.append(seq)\n",
    "        seq_labels.append(label)\n",
    "    return np.array(seqs), np.array(seq_labels)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # directories\n",
    "    if os.name == 'nt':\n",
    "        data_dir = 'C:/Users/jed95/Documents/GitHub/anomaly_detection/dataset/yahoo_s5/A2Benchmark/'\n",
    "    else:\n",
    "        data_dir = '/home/adlink3/Downloads/yahoo_s5/A2Benchmark/'\n",
    "    window_size = 10\n",
    "\n",
    "    # load csvs\n",
    "    files = [os.path.join(data_dir,f) for f in os.listdir(data_dir) if f.endswith('.csv')]\n",
    "    dfs = [pd.read_csv(f) for f in files]\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    df.dropna(inplace=True)\n",
    "    scaler = MinMaxScaler()\n",
    "    df['value'] = scaler.fit_transform(df['value'].values.reshape(-1,1))\n",
    "\n",
    "    # sequences\n",
    "    X, y = create_sequences(df['value'].values, df['is_anomaly'].values, window_size)\n",
    "    # split normals and anomalies\n",
    "    normals = np.where(y==0)[0]\n",
    "    anomalies = np.where(y==1)[0]\n",
    "    Xn, yn = X[normals], y[normals]\n",
    "    Xa, ya = X[anomalies], y[anomalies]\n",
    "    # train/val/test splits\n",
    "    Xn_train, Xn_tmp, yn_train, yn_tmp = train_test_split(Xn, yn, test_size=0.6, random_state=42, stratify=yn)\n",
    "    Xn_val, Xn_test, yn_val, yn_test   = train_test_split(Xn_tmp, yn_tmp, test_size=0.5, random_state=42, stratify=yn_tmp)\n",
    "    Xa_train, Xa_tmp, ya_train, ya_tmp = train_test_split(Xa, ya, test_size=0.6, random_state=42, stratify=ya)\n",
    "    Xa_val, Xa_test, ya_val, ya_test   = train_test_split(Xa_tmp, ya_tmp, test_size=0.5, random_state=42, stratify=ya_tmp)\n",
    "    # combine\n",
    "    X_train = np.concatenate([Xn_train, Xa_train], axis=0)\n",
    "    y_train = np.concatenate([yn_train, ya_train], axis=0)\n",
    "    X_val   = np.concatenate([Xn_val,   Xa_val],   axis=0)\n",
    "    y_val   = np.concatenate([yn_val,   ya_val],   axis=0)\n",
    "    X_test  = np.concatenate([Xn_test,  Xa_test],  axis=0)\n",
    "    y_test  = np.concatenate([yn_test,  ya_test],  axis=0)\n",
    "    print(\"Training data shape:\", X_train.shape, y_train.shape)\n",
    "    print(\"Validation data shape:\", X_val.shape, y_val.shape)\n",
    "    print(\"Test data shape:\", X_test.shape, y_test.shape)\n",
    "    # shuffle\n",
    "    X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
    "    X_val,   y_val   = shuffle(X_val,   y_val,   random_state=42)\n",
    "    X_test,  y_test  = shuffle(X_test,  y_test,  random_state=42)\n",
    "\n",
    "    # tensors\n",
    "    X_train_t = torch.tensor(X_train).unsqueeze(1).float()\n",
    "    y_train_t = torch.tensor(y_train).long()\n",
    "    X_val_t   = torch.tensor(X_val).unsqueeze(1).float()\n",
    "    y_val_t   = torch.tensor(y_val).long()\n",
    "    X_test_t  = torch.tensor(X_test).unsqueeze(1).float()\n",
    "    y_test_t  = torch.tensor(y_test).long()\n",
    "\n",
    "    # save uncompressed\n",
    "    np.save('X_train_t.npy', X_train_t)\n",
    "    size_un = get_size('X_train_t.npy')\n",
    "\n",
    "    # dataloader\n",
    "    train_ds = torch.utils.data.TensorDataset(X_train_t)\n",
    "    loader = torch.utils.data.DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "\n",
    "    # model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = Conv1dAutoencoder(input_length=window_size, latent_dim=10).to(device)\n",
    "    opt = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    # train\n",
    "    model.train()\n",
    "    for ep in range(5):\n",
    "        tot=0\n",
    "        for bx, in loader:\n",
    "            bx = bx.to(device)\n",
    "            recon, _ = model(bx)\n",
    "            loss = loss_fn(recon, bx)\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "            tot+=loss.item()\n",
    "        print(f'Epoch {ep+1} Loss {tot/len(loader):.4f}')\n",
    "\n",
    "    # infer\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        recon, z = model(X_train_t.to(device))\n",
    "\n",
    "        z = z.cpu().numpy()\n",
    "    #print(summary(\n",
    "    #model,\n",
    "    #input_size=(32, 1, window_size),\n",
    "    #))\n",
    "    # save compressed and append labels\n",
    "    np.save('compressed.npy', z)\n",
    "    dfz = pd.DataFrame(z, columns=[f'z{i}' for i in range(z.shape[1])])\n",
    "    dfz['label'] = y_train\n",
    "    dfz.to_csv('compressed_with_labels.csv', index=False)\n",
    "\n",
    "    # sizes\n",
    "    s_c_np = get_size('compressed.npy')\n",
    "    s_u = size_un\n",
    "    ratio = s_c_np / s_u if s_u else float('nan')\n",
    "    report = f\"Uncompressed: {s_u} bytes\\nCompressed: {s_c_np} bytes\\nRatio: {ratio:.3f}\\n\"\n",
    "    print(report)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
