{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "160e0d6c",
   "metadata": {},
   "source": [
    "# Demonstration of anomaly detection with CVAE using DASHlink data\n",
    "\n",
    "**Author: Milad Memarzadeh (milad.memarzadeh@nasa.gov)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831f2c0d",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a6137f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix, precision_recall_fscore_support, roc_curve, precision_recall_curve, average_precision_score\n",
    "from sklearn.utils import shuffle\n",
    "import copy\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b697530",
   "metadata": {},
   "source": [
    "# Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb4fb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the dataset\n",
    "data_dir = 'C:/Users/jed95/Documents/GitHub/anomaly_detection/dataset/yahoo_s5/A2Benchmark'  # Adjust the path as necessary\n",
    "\n",
    "# Get the list of all CSV files in the directory\n",
    "file_list = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.csv')]\n",
    "\n",
    "# Load all files into a single DataFrame\n",
    "df_list = []\n",
    "for file in file_list:\n",
    "    df = pd.read_csv(file)\n",
    "    df_list.append(df)\n",
    "\n",
    "data = pd.concat(df_list, ignore_index=True)\n",
    "print(\"Data shape:\", data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9599316a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values:\", data.isnull().sum())\n",
    "\n",
    "# For simplicity, drop missing values (if any)\n",
    "data.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4345488d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "data['value'] = scaler.fit_transform(data['value'].values.reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f665b01",
   "metadata": {},
   "source": [
    "# Create Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b216f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(values, labels, sequence_length):\n",
    "    sequences = []\n",
    "    seq_labels = []\n",
    "    for i in range(len(values) - sequence_length + 1):\n",
    "        seq = values[i:i + sequence_length]\n",
    "        label = labels[i + sequence_length - 1]  # Label of the last element in the sequence\n",
    "        sequences.append(seq)\n",
    "        seq_labels.append(label)\n",
    "    return np.array(sequences), np.array(seq_labels)\n",
    "\n",
    "sequence_length = 10  # You can adjust this \n",
    "values = data['value'].values\n",
    "labels = data['is_anomaly'].values  # Assuming 'is_anomaly' is the label column\n",
    "\n",
    "sequences, seq_labels = create_sequences(values, labels, sequence_length)\n",
    "print(\"Sequences shape:\", sequences.shape)\n",
    "print(\"Sequence labels shape:\", seq_labels.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4a0c45",
   "metadata": {},
   "source": [
    "# Split data into labeled and unlabeled sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6284fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indices for normal and anomalous sequences\n",
    "normal_indices = np.where(seq_labels == 0)[0]\n",
    "anomalous_indices = np.where(seq_labels == 1)[0]\n",
    "\n",
    "# Extract normal and anomalous sequences\n",
    "normal_sequences = sequences[normal_indices]\n",
    "normal_labels = seq_labels[normal_indices]\n",
    "\n",
    "anomalous_sequences = sequences[anomalous_indices]\n",
    "anomalous_labels = seq_labels[anomalous_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb09988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split normal data\n",
    "X_train_normal, X_test_normal, y_train_normal, y_test_normal = train_test_split(\n",
    "    normal_sequences, normal_labels, test_size=0.4, random_state=42, stratify=normal_labels)\n",
    "X_train_normal, X_val_normal, y_train_normal, y_val_normal = train_test_split(\n",
    "    X_train_normal, y_train_normal, test_size=0.25, random_state=42, stratify=y_train_normal)\n",
    "\n",
    "# Split anomalous data\n",
    "X_train_anomalous, X_test_anomalous, y_train_anomalous, y_test_anomalous = train_test_split(\n",
    "    anomalous_sequences, anomalous_labels, test_size=0.4, random_state=42, stratify=anomalous_labels)\n",
    "X_train_anomalous, X_val_anomalous, y_train_anomalous, y_val_anomalous = train_test_split(\n",
    "    X_train_anomalous, y_train_anomalous, test_size=0.25, random_state=42, stratify=y_train_anomalous)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65f3dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine training data\n",
    "X_train = np.concatenate([X_train_normal, X_train_anomalous], axis=0)\n",
    "y_train = np.concatenate([y_train_normal, y_train_anomalous], axis=0)\n",
    "# Combine validation data\n",
    "X_val = np.concatenate([X_val_normal, X_val_anomalous], axis=0)\n",
    "y_val = np.concatenate([y_val_normal, y_val_anomalous], axis=0)\n",
    "# Combine test data\n",
    "X_test = np.concatenate([X_test_normal, X_test_anomalous], axis=0)\n",
    "y_test = np.concatenate([y_test_normal, y_test_anomalous], axis=0)\n",
    "print(\"Training data shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation data shape:\", X_val.shape, y_val.shape)\n",
    "print(\"Test data shape:\", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28af8376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle data\n",
    "\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
    "#X_val, y_val = shuffle(X_val, y_val, random_state=42)\n",
    "X_test, y_test = shuffle(X_test, y_test, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db1beef",
   "metadata": {},
   "source": [
    "# Convert Data to Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a03c958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert training data to tensors\n",
    "X_train_tensor = torch.tensor(X_train).float()\n",
    "y_train_tensor = torch.tensor(y_train).long()\n",
    "# Convert validation data to tensors\n",
    "X_val_tensor = torch.tensor(X_val).float()\n",
    "y_val_tensor = torch.tensor(y_val).long()\n",
    "# Convert test data to tensors\n",
    "X_test_tensor = torch.tensor(X_test).float()\n",
    "y_test_tensor = torch.tensor(y_test).long()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50dc79a",
   "metadata": {},
   "source": [
    "# Create Data Loaders for Labeled and Unlabeled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af172bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels  # Now labels are provided for all data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = self.data[idx]\n",
    "        y = self.labels[idx]\n",
    "        return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21a408d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # Adjust as needed\n",
    "\n",
    "# Training data loader\n",
    "train_dataset = TimeSeriesDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False) #TODO adjust Shuffle training data? \n",
    "\n",
    "# Validation data loader\n",
    "val_dataset = TimeSeriesDataset(X_val_tensor, y_val_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Test data loader\n",
    "test_dataset = TimeSeriesDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73e288d",
   "metadata": {},
   "source": [
    "# Instantiate and Train the Conditional VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0a92ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.modelsCondVAE import *\n",
    "from source.utilsCondVAEs5 import *\n",
    "model_name=\"CondVAE_model\"\n",
    "save_dir=\"./\"\n",
    "# Instantiate the model\n",
    "latent_dim = 10  # Adjust as needed\n",
    "num_param = 10    # Since we have univariate time series\n",
    "window_size = 1\n",
    "num_classes = 2  # Normal and Anomaly\n",
    "scale_flag = 0   # Use Sigmoid activation in the decoder\n",
    "\n",
    "model = VAE(latent_dim, num_param, window_size, num_classes, scale_flag).to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 2  # Adjust as needed\n",
    "model = train_model(\n",
    "    model,\n",
    "    optimizer,\n",
    "    train_loader,\n",
    "    num_epochs=num_epochs,\n",
    "    save=True,\n",
    "    save_dir=save_dir,\n",
    "    model_name=model_name)\n",
    "\n",
    "# Serialize the model's state_dict to a bytes buffer\n",
    "import io\n",
    "buffer = io.BytesIO()\n",
    "torch.save(model.state_dict(), buffer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ea30df",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_traj = np.load(save_dir+model_name+\"_training_loss.npz\")\n",
    "\n",
    "total_loss = training_traj['training_total_loss']\n",
    "rec_loss = training_traj['training_rec_loss']\n",
    "kl_loss = training_traj['training_kl_loss']\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 4))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Total Loss\", fontsize=12)\n",
    "plt.plot(range(num_epochs), total_loss)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Reconstruction Loss\", fontsize=12)\n",
    "plt.plot(range(num_epochs), rec_loss)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.xlabel(\"Epochs of training\", fontsize=12)\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"KL Loss\", fontsize=12)\n",
    "plt.plot(range(num_epochs), kl_loss)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019aa278",
   "metadata": {},
   "source": [
    "# Initialize 3 new models for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86116d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deserialize the model from the buffer\n",
    "buffer.seek(0)  # Reset buffer pointer to the beginning\n",
    "model_medium = VAE(latent_dim, num_param, window_size, num_classes, scale_flag).to(device)\n",
    "model_medium.load_state_dict(torch.load(buffer))\n",
    "model_medium = model_medium.to(device)\n",
    "\n",
    "buffer.seek(0)  # Reset buffer pointer to the beginning\n",
    "model_strong = VAE(latent_dim, num_param, window_size, num_classes, scale_flag).to(device)\n",
    "model_strong.load_state_dict(torch.load(buffer))\n",
    "model_strong = model_strong.to(device)\n",
    "buffer.seek(0)  # Reset buffer pointer to the beginning\n",
    "model_random = VAE(latent_dim, num_param, window_size, num_classes, scale_flag).to(device)\n",
    "model_random.load_state_dict(torch.load(buffer))\n",
    "model_random = model_random.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7809e18a",
   "metadata": {},
   "source": [
    "# Verify the 3 new models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12f6aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a flag\n",
    "models_identical = True\n",
    "original_state_dict = model.state_dict()\n",
    "loaded_state_dict = model_medium.state_dict()\n",
    "# Compare each parameter\n",
    "for key in original_state_dict:\n",
    "    original_param = original_state_dict[key]\n",
    "    loaded_param = loaded_state_dict[key]\n",
    "    if not torch.equal(original_param, loaded_param):\n",
    "        print(f\"Mismatch found at layer: {key}\")\n",
    "        models_identical = False\n",
    "        break\n",
    "\n",
    "if models_identical:\n",
    "    print(\"The saved and loaded models are identical.\")\n",
    "else:\n",
    "    print(\"The models are not identical.\")\n",
    "# Initialize a flag\n",
    "models_identical = True\n",
    "original_state_dict = model_random.state_dict()\n",
    "loaded_state_dict = model_medium.state_dict()\n",
    "# Compare each parameter\n",
    "for key in original_state_dict:\n",
    "    original_param = original_state_dict[key]\n",
    "    loaded_param = loaded_state_dict[key]\n",
    "    if not torch.equal(original_param, loaded_param):\n",
    "        print(f\"Mismatch found at layer: {key}\")\n",
    "        models_identical = False\n",
    "        break\n",
    "\n",
    "if models_identical:\n",
    "    print(\"The saved and loaded models are identical.\")\n",
    "else:\n",
    "    print(\"The models are not identical.\")\n",
    "# Initialize a flag\n",
    "models_identical = True\n",
    "original_state_dict = model.state_dict()\n",
    "loaded_state_dict = model_strong.state_dict()\n",
    "# Compare each parameter\n",
    "for key in original_state_dict:\n",
    "    original_param = original_state_dict[key]\n",
    "    loaded_param = loaded_state_dict[key]\n",
    "    if not torch.equal(original_param, loaded_param):\n",
    "        print(f\"Mismatch found at layer: {key}\")\n",
    "        models_identical = False\n",
    "        break\n",
    "\n",
    "if models_identical:\n",
    "    print(\"The saved and loaded models are identical.\")\n",
    "else:\n",
    "    print(\"The models are not identical.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06774dd",
   "metadata": {},
   "source": [
    "# Evaluate the Model and Detect Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eed4ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_anomaly_scores(model, data_loader):\n",
    "    model.eval()\n",
    "    anomaly_scores = []\n",
    "    true_labels = []\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            X_batch = X_batch.unsqueeze(1).to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            x_rec, class_logits = model(X_batch)\n",
    "            y_probs = F.softmax(class_logits, dim=1)\n",
    "\n",
    "            # Reconstruction error\n",
    "            rec_error = torch.mean((X_batch - x_rec) ** 2, dim=[1, 2])\n",
    "\n",
    "            # Classification probability for anomaly class\n",
    "            anomaly_prob = y_probs[:, 1]\n",
    "\n",
    "            # Combine scores\n",
    "            anomaly_score = rec_error * anomaly_prob\n",
    "\n",
    "            anomaly_scores.extend(anomaly_score.cpu().numpy())\n",
    "            true_labels.extend(y_batch.cpu().numpy())\n",
    "            predictions.extend(torch.argmax(class_logits, dim=1).cpu().numpy())\n",
    "\n",
    "    return np.array(anomaly_scores), np.array(true_labels), np.array(predictions)\n",
    "\n",
    "# Compute anomaly scores\n",
    "train_anomaly_scores, train_true_labels, train_predictions = compute_anomaly_scores(model, train_loader)\n",
    "val_anomaly_scores, val_true_labels, val_predictions = compute_anomaly_scores(model, val_loader)\n",
    "test_anomaly_scores, test_true_labels, test_predictions = compute_anomaly_scores(model, test_loader)\n",
    "#print(test_anomaly_scores)\n",
    "info = precision_recall_fscore_support(test_true_labels, test_predictions, pos_label=1)\n",
    "print(\"Precision = {}%, recall = {}% and F1-score = {}%\".format(np.round(info[0][1]*100, 2),\n",
    "                                                                np.round(info[1][1]*100, 2),\n",
    "                                                                np.round(info[2][1]*100, 2)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219ea362",
   "metadata": {},
   "source": [
    "# Retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12dc8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine thresholds\n",
    "lower_percentile = 30\n",
    "upper_percentile = 70\n",
    "lower_threshold = np.percentile(val_anomaly_scores, lower_percentile)\n",
    "upper_threshold = np.percentile(val_anomaly_scores, upper_percentile)\n",
    "\n",
    "# Select medium-score data\n",
    "medium_score_indices = np.where((val_anomaly_scores >= lower_threshold) & (val_anomaly_scores <= upper_threshold))[0]\n",
    "X_medium = X_val[medium_score_indices]\n",
    "y_medium = y_val[medium_score_indices]\n",
    "X_medium_tensor = torch.tensor(X_medium).float()\n",
    "y_medium_tensor = torch.tensor(y_medium).long()\n",
    "# data loader\n",
    "medium_dataset = TimeSeriesDataset(X_medium_tensor, y_medium_tensor)\n",
    "medium_loader = DataLoader(medium_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4347817d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine thresholds\n",
    "lower_percentile = 20\n",
    "upper_percentile = 80\n",
    "lower_threshold = np.percentile(val_anomaly_scores, lower_percentile)\n",
    "upper_threshold = np.percentile(val_anomaly_scores, upper_percentile)\n",
    "\n",
    "# Select strong-score data\n",
    "strong_score_indices = np.where((val_anomaly_scores <= lower_threshold) & (val_anomaly_scores >= upper_threshold))[0]\n",
    "X_strong = X_val[strong_score_indices]\n",
    "y_strong = y_val[strong_score_indices]\n",
    "X_strong_tensor = torch.tensor(X_strong).float()\n",
    "y_strong_tensor = torch.tensor(y_strong).long()\n",
    "# data loader\n",
    "strong_dataset = TimeSeriesDataset(X_strong_tensor, y_strong_tensor)\n",
    "strong_loader = DataLoader(strong_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b63f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the fraction of data to select\n",
    "sample_fraction = 0.4\n",
    "total_samples = len(val_anomaly_scores)\n",
    "sample_size = int(sample_fraction * total_samples)\n",
    "\n",
    "# Randomly select unique indices without replacement\n",
    "random_score_indices = np.random.choice(total_samples, size=sample_size, replace=False)\n",
    "\n",
    "# Select the corresponding data and labels\n",
    "X_random = X_val[random_score_indices]\n",
    "y_random = y_val[random_score_indices]\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_random_tensor = torch.tensor(X_random).float()\n",
    "y_random_tensor = torch.tensor(y_random).long()\n",
    "\n",
    "# Create the dataset and data loader\n",
    "random_dataset = TimeSeriesDataset(X_random_tensor, y_random_tensor)\n",
    "random_loader = DataLoader(random_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e09a81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FInetune the model\n",
    "optimizer = optim.Adam(model_medium.parameters(), lr=1e-4)\n",
    "#optimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-7)\n",
    "model_medium = train_model_full(model_medium, optimizer, medium_loader, num_epochs=5)\n",
    "models_identical = True\n",
    "original_state_dict = model.state_dict()\n",
    "loaded_state_dict = model_medium.state_dict()\n",
    "# Compare each parameter\n",
    "for key in original_state_dict:\n",
    "    original_param = original_state_dict[key]\n",
    "    loaded_param = loaded_state_dict[key]\n",
    "    if not torch.equal(original_param, loaded_param):\n",
    "        print(f\"Mismatch found at layer: {key}\")\n",
    "        models_identical = False\n",
    "        break\n",
    "\n",
    "if models_identical:\n",
    "    print(\"The saved and loaded models are identical.\")\n",
    "else:\n",
    "    print(\"The models are not identical.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba323de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model_strong.parameters(), lr=1e-4)\n",
    "model_strong = train_model_full(model_strong, optimizer, strong_loader, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c4bbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model_random.parameters(), lr=1e-4)\n",
    "model_random = train_model_full(model_random, optimizer, random_loader, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b813da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine data\n",
    "expanded_medium_data = np.concatenate((X_train, X_medium), axis=0)\n",
    "expanded_medium_labels = np.concatenate((y_train, y_medium), axis=0)\n",
    "X_expanded_medium_tensor = torch.tensor(expanded_medium_data).float()\n",
    "y_expanded_medium_tensor = torch.tensor(expanded_medium_labels).long()\n",
    "# Create new dataset and loader\n",
    "expanded_medium_dataset = TimeSeriesDataset(X_expanded_medium_tensor, y_expanded_medium_tensor)\n",
    "expanded_medium_loader = DataLoader(expanded_medium_dataset, batch_size=batch_size, shuffle=False)\n",
    "model2_medium = VAE(latent_dim, num_param, window_size, num_classes, scale_flag).to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model2_medium.parameters(), lr=1e-3)\n",
    "model2_medium = train_model_full(model2_medium, optimizer, expanded_medium_loader, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89743f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_strong_data = np.concatenate((X_train, X_strong), axis=0)\n",
    "expanded_strong_labels = np.concatenate((y_train, y_strong), axis=0)\n",
    "X_expanded_strong_tensor = torch.tensor(expanded_strong_data).float()\n",
    "y_expanded_strong_tensor = torch.tensor(expanded_strong_labels).long()\n",
    "# Create new dataset and loader\n",
    "expanded_strong_dataset = TimeSeriesDataset(X_expanded_strong_tensor, y_expanded_strong_tensor)\n",
    "expanded_strong_loader = DataLoader(expanded_strong_dataset, batch_size=batch_size, shuffle=False)\n",
    "model2_strong = VAE(latent_dim, num_param, window_size, num_classes, scale_flag).to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model2_strong.parameters(), lr=1e-3)\n",
    "model2_strong = train_model_full(model2_strong, optimizer, expanded_strong_loader, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6823b7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_random_data = np.concatenate((X_train, X_random), axis=0)\n",
    "expanded_random_labels = np.concatenate((y_train, y_random), axis=0)\n",
    "X_expanded_random_tensor = torch.tensor(expanded_random_data).float()\n",
    "y_expanded_random_tensor = torch.tensor(expanded_random_labels).long()\n",
    "# Create new dataset and loader\n",
    "expanded_random_dataset = TimeSeriesDataset(X_expanded_random_tensor, y_expanded_random_tensor)\n",
    "expanded_random_loader = DataLoader(expanded_random_dataset, batch_size=batch_size, shuffle=False)\n",
    "model2_random = VAE(latent_dim, num_param, window_size, num_classes, scale_flag).to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model2_random.parameters(), lr=1e-3)\n",
    "model2_random = train_model_full(model2_random, optimizer, expanded_random_loader, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7369e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "# Assuming you have already defined and initialized your models:\n",
    "# model, model_medium, model_random, model_strong\n",
    "\n",
    "# Dictionary of models with their names for easy reference\n",
    "models = {\n",
    "    'model': model,\n",
    "    'model_medium': model_medium,\n",
    "    'model_random': model_random,\n",
    "    'model_strong': model_strong,\n",
    "    'model2_medium': model2_medium,\n",
    "    'model2_random': model2_random,\n",
    "    'model2_strong': model2_strong\n",
    "}\n",
    "\n",
    "# Generate all possible unique pairs of models\n",
    "model_pairs = list(combinations(models.items(), 2))\n",
    "\n",
    "# Lists to keep track of identical and different model pairs\n",
    "identical_pairs = []\n",
    "different_pairs = []\n",
    "\n",
    "# Function to compare two models\n",
    "def compare_models(model1, model2, name1, name2):\n",
    "    state_dict1 = model1.state_dict()\n",
    "    state_dict2 = model2.state_dict()\n",
    "    \n",
    "    # First, check if both models have the same set of keys (layers)\n",
    "    keys1 = set(state_dict1.keys())\n",
    "    keys2 = set(state_dict2.keys())\n",
    "    \n",
    "    if keys1 != keys2:\n",
    "        missing_in_1 = keys2 - keys1\n",
    "        missing_in_2 = keys1 - keys2\n",
    "        if missing_in_1:\n",
    "            \n",
    "            print(f\"Model '{name1}' is missing layers: {missing_in_1}\")\n",
    "        if missing_in_2:\n",
    "            \n",
    "            print(f\"Model '{name2}' is missing layers: {missing_in_2}\")\n",
    "        return False  # Layers mismatch implies models are different\n",
    "    \n",
    "    # Compare each parameter tensor\n",
    "    for key in state_dict1:\n",
    "        param1 = state_dict1[key]\n",
    "        param2 = state_dict2[key]\n",
    "        \n",
    "        if not torch.equal(param1, param2):\n",
    "            #print(f\"Mismatch found in layer '{key}' between '{name1}' and '{name2}'.\")\n",
    "            return False  # Found a mismatch\n",
    "    \n",
    "    return True  # All parameters match\n",
    "\n",
    "# Iterate through each pair and compare\n",
    "for (name1, model1), (name2, model2_medium) in model_pairs:\n",
    "    print(f\"Comparing '{name1}' with '{name2}':\")\n",
    "    are_identical = compare_models(model1, model2_medium, name1, name2)\n",
    "    \n",
    "    if are_identical:\n",
    "        identical_pairs.append((name1, name2))\n",
    "        print(f\"--> '{name1}' and '{name2}' are IDENTICAL.\\n\")\n",
    "    else:\n",
    "        different_pairs.append((name1, name2))\n",
    "        print(f\"--> '{name1}' and '{name2}' are DIFFERENT.\\n\")\n",
    "\n",
    "# Summary of results\n",
    "print(\"=== Comparison Summary ===\\n\")\n",
    "\n",
    "if identical_pairs:\n",
    "    print(\"Identical Model Pairs:\")\n",
    "    for name1, name2 in identical_pairs:\n",
    "        print(f\" - {name1} and {name2}\")\n",
    "else:\n",
    "    print(\"No identical model pairs found.\")\n",
    "\n",
    "print()\n",
    "\n",
    "if different_pairs:\n",
    "    print(\"Different Model Pairs:\")\n",
    "    for name1, name2 in different_pairs:\n",
    "        print(f\" - {name1} and {name2}\")\n",
    "else:\n",
    "    print(\"No different model pairs found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bc87d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_anomaly_scores, test_true_labels, test_predictions = compute_anomaly_scores(model_medium, test_loader)\n",
    "info = precision_recall_fscore_support(test_true_labels, test_predictions, pos_label=1)\n",
    "print(\"Precision = {}%, recall = {}% and F1-score = {}%\".format(np.round(info[0][1]*100, 2),\n",
    "                                                                np.round(info[1][1]*100, 2),\n",
    "                                                                np.round(info[2][1]*100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5d4b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_anomaly_scores, test_true_labels, test_predictions = compute_anomaly_scores(model_strong, test_loader)\n",
    "info = precision_recall_fscore_support(test_true_labels, test_predictions, pos_label=1)\n",
    "print(\"Precision = {}%, recall = {}% and F1-score = {}%\".format(np.round(info[0][1]*100, 2),\n",
    "                                                                np.round(info[1][1]*100, 2),\n",
    "                                                                np.round(info[2][1]*100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a12403c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_anomaly_scores, test_true_labels, test_predictions = compute_anomaly_scores(model_random, test_loader)\n",
    "info = precision_recall_fscore_support(test_true_labels, test_predictions, pos_label=1)\n",
    "print(\"Precision = {}%, recall = {}% and F1-score = {}%\".format(np.round(info[0][1]*100, 2),\n",
    "                                                                np.round(info[1][1]*100, 2),\n",
    "                                                                np.round(info[2][1]*100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecb4bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_anomaly_scores, test_true_labels, test_predictions = compute_anomaly_scores(model2_medium, test_loader)\n",
    "info = precision_recall_fscore_support(test_true_labels, test_predictions, pos_label=1)\n",
    "print(\"Precision = {}%, recall = {}% and F1-score = {}%\".format(np.round(info[0][1]*100, 2),\n",
    "                                                                np.round(info[1][1]*100, 2),\n",
    "                                                                np.round(info[2][1]*100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629e15a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_anomaly_scores, test_true_labels, test_predictions = compute_anomaly_scores(model2_strong, test_loader)\n",
    "info = precision_recall_fscore_support(test_true_labels, test_predictions, pos_label=1)\n",
    "print(\"Precision = {}%, recall = {}% and F1-score = {}%\".format(np.round(info[0][1]*100, 2),\n",
    "                                                                np.round(info[1][1]*100, 2),\n",
    "                                                                np.round(info[2][1]*100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5967fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_anomaly_scores, test_true_labels, test_predictions = compute_anomaly_scores(model2_random, test_loader)\n",
    "info = precision_recall_fscore_support(test_true_labels, test_predictions, pos_label=1)\n",
    "print(\"Precision = {}%, recall = {}% and F1-score = {}%\".format(np.round(info[0][1]*100, 2),\n",
    "                                                                np.round(info[1][1]*100, 2),\n",
    "                                                                np.round(info[2][1]*100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc0aa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the scale factor\n",
    "scale = 2  # You can adjust this value\n",
    "# Calculate the threshold\n",
    "threshold = np.mean(train_anomaly_scores) + scale * np.std(train_anomaly_scores)\n",
    "print(f\"Anomaly Detection Threshold: {threshold}\")\n",
    "# Classify test data based on threshold\n",
    "threshold_predictions = (test_anomaly_scores > threshold).astype(int)\n",
    "\n",
    "# Evaluate threshold-based predictions\n",
    "print(\"Threshold-based Classification Report:\")\n",
    "print(classification_report(test_true_labels, threshold_predictions, target_names=['Normal', 'Anomaly']))\n",
    "\n",
    "# Confusion matrix\n",
    "cm_threshold = confusion_matrix(test_true_labels, threshold_predictions)\n",
    "print(\"Threshold-based Confusion Matrix:\")\n",
    "print(cm_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d281fb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(train_anomaly_scores[train_true_labels == 0], bins=50, alpha=0.6, label='Normal')\n",
    "plt.hist(train_anomaly_scores[train_true_labels == 1], bins=50, alpha=0.6, label='Anomaly')\n",
    "plt.title('Anomaly Score Distribution')\n",
    "plt.xlabel('Anomaly Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01f7b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_anomaly_scores_scatter(anomaly_scores, true_labels, threshold):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(range(len(anomaly_scores)), anomaly_scores, c=true_labels, cmap='coolwarm', label='Data Point')\n",
    "    plt.axhline(threshold, color='red', linestyle='--', label='Threshold')\n",
    "    plt.title('Anomaly Scores Scatter Plot')\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Anomaly Score')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function\n",
    "plot_anomaly_scores_scatter(test_anomaly_scores, test_true_labels, threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4583f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set plot style\n",
    "sns.set(style='whitegrid')\n",
    "def plot_anomaly_score_histogram(anomaly_scores, true_labels, threshold):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(anomaly_scores[true_labels == 0], bins=50, color='green', label='Normal', kde=True, stat='density')\n",
    "    sns.histplot(anomaly_scores[true_labels == 1], bins=50, color='red', label='Anomaly', kde=True, stat='density')\n",
    "    plt.axvline(threshold, color='blue', linestyle='--', label='Threshold')\n",
    "    plt.title('Histogram of Anomaly Scores')\n",
    "    plt.xlabel('Anomaly Score')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function\n",
    "plot_anomaly_score_histogram(test_anomaly_scores, test_true_labels, threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1997ea62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Compute AUC-ROC\n",
    "auc_score = roc_auc_score(test_true_labels, test_anomaly_scores)\n",
    "print(f\"AUC-ROC Score: {auc_score:.4f}\")\n",
    "\n",
    "# Compute Precision-Recall Curve\n",
    "precision, recall, thresholds = precision_recall_curve(test_true_labels, test_anomaly_scores)\n",
    "ap_score = average_precision_score(test_true_labels, test_anomaly_scores)\n",
    "print(f\"Average Precision Score: {ap_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e176694",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(test_true_labels, test_anomaly_scores)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'AUC = {auc_score:.4f}')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.title('ROC Curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3706a868",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(test_true_labels, test_predictions, target_names=['Normal', 'Anomaly']))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(test_true_labels, test_predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Compute AUC-ROC\n",
    "auc_score = roc_auc_score(test_true_labels, test_anomaly_scores)\n",
    "print(f\"AUC-ROC Score: {auc_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9979b673",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(test_anomaly_scores[test_true_labels == 0], bins=50, alpha=0.6, label='Normal')\n",
    "plt.hist(test_anomaly_scores[test_true_labels == 1], bins=50, alpha=0.6, label='Anomaly')\n",
    "plt.axvline(threshold, color='red', linestyle='--', label='Threshold')\n",
    "plt.title('Anomaly Score Distribution with Threshold')\n",
    "plt.xlabel('Anomaly Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
